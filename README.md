# [â€¢_â€¢] Hands for Bots

<div align="center">

![Hands for Bots](https://img.shields.io/badge/[â€¢__â€¢]-Hands_for_Bots-purple?style=social) &nbsp; ![Conversational User Interface](https://img.shields.io/badge/ðŸ—£-Conversational_UI-purple?style=social) &nbsp; ![Front-end](https://img.shields.io/badge/ðŸ“º-Front_end-purple?style=social)

[![JavaScript](https://img.shields.io/badge/javascript-%23323330.svg?style=for-the-badge&logo=javascript&logoColor=%23F7DF1E)](https://www.javascript.com) &nbsp; [![MIT License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge&color=%23750014)](./LICENSE.md)

ðŸ˜© [![GitHub contributors](https://img.shields.io/github/contributors/alexlana/handsforbots)](https://GitHub.com/alexlana/handsforbots/graphs/contributors/) ðŸ˜¥

</div>

For now the **Hands for Bots** is a conversational UI library for browsers. It gives to chatbots / assistants the hability to interact with GUI and other user interfaces through functions calling, and receive inputs from the UIs. 

Hands for Bots uses a modular archtecture based on plugins to grant a high level of flexibility, modularity and extensibility to build chatbots' front end. Event triggers and listeners connects the core to plugins. To call external functions, we should call the function directly, this not depends on the internal architecture.

**Important:** this library don't give to assistants the hability to "view" the screen and do things in your computer.

## Table of contents

- [Get started](./docs/getstarted.md)
- [Development](./docs/development.md)

## Acknowledgment

Grateful for the authors of [these third-party libraries](./NOTICE.md)

## Roadmap

There is an intention to improve the quality of the Hands for Bots' code and develo ready-to-use features for virtual reality and conversational applications, perhaps even in wearable technologies. That's a long way off, but let's get it.

